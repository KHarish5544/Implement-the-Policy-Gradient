import numpy as np #package 

class REINFORCEAgent:# class
    def __init__(self, num_actions, num_states, gamma=0.99, learning_rate=0.01):
         # gamma is discount factor for finding future reward
        self.num_actions = num_actions
        self.num_states = num_states
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.policy = np.zeros((num_states, num_actions))
    
    def get_action(self, state): #return action for current policy
        action_probs = self._softmax(self.policy[state])
        #Accesses the policy for the current state
        #softmax function that takes as input a vector of real numbers and returns a vector of probabilities
        return np.random.choice(self.num_actions, p=action_probs)
    
    def train(self, episode):
        states, actions, rewards = zip(*episode)
        returns = self._calculate_returns(rewards)
        for t, (state, action) in enumerate(zip(states, actions)):
            delta = returns[t] - self.policy[state, action]
            self.policy[state, action] += self.learning_rate * delta
        print("Episode training complete.")
            #enumerate built-in function that allows you to loop over an iterable (such as a list, tuple, or string)
    def _calculate_returns(self, rewards):
        G = 0
        returns = []
        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        return returns
    
    def _softmax(self, x):
        exp_values = np.exp(x - np.max(x)) #exponential 
        return exp_values / np.sum(exp_values)

# Simple environment
class SimpleEnvironment:# class
    def __init__(self, num_states, num_actions):
        self.num_states = num_states
        self.num_actions = num_actions
    
    def reset(self):
        return 0
    
    def step(self, state, action):
        new_state = max(0, min(self.num_states - 1, state + (action * 2 - 1)))
        reward = 1 if new_state == self.num_states - 1 else 0
        return new_state, reward

# Training loop
num_states = 5
num_actions = 2
num_episodes = 1000
env = SimpleEnvironment(num_states, num_actions)
agent = REINFORCEAgent(num_actions, num_states)

for episode_num in range(num_episodes):
    state = env.reset()
    episode = []
    total_reward = 0  # Track total reward for the episode
    done = False
    while not done:
        action = agent.get_action(state)
        next_state, reward = env.step(state, action)
        total_reward += reward  # Accumulate reward for the episode
        episode.append((state, action, reward))
        state = next_state
        done = next_state == num_states - 1
    agent.train(episode)
    print("Episode:", episode_num + 1, "Total Reward:", total_reward)  # Print total reward for the episode
